@inproceedings{Goodfellow14,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {{ICLR}},
  year      = {2015}
}

@inproceedings{Szegedy13,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  title     = {Intriguing properties of neural networks},
  booktitle = {{ICLR}},
  year      = {2014}
}
@inproceedings{Guo18,
  author    = {Chuan Guo and
               Mayank Rana and
               Moustapha Ciss{\'{e}} and
               Laurens van der Maaten},
  title     = {Countering Adversarial Images using Input Transformations},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2018}
}

@article{HintonVD15,
  author    = {Geoffrey E. Hinton and
               Oriol Vinyals and
               Jeffrey Dean},
  title     = {Distilling the Knowledge in a Neural Network},
  journal   = {CoRR},
  volume    = {abs/1503.02531},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.02531},
  archivePrefix = {arXiv},
  eprint    = {1503.02531},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HintonVD15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{efros2001image,
  title={Image quilting for texture synthesis and transfer},
  author={Efros, Alexei A and Freeman, William T},
  booktitle={Proceedings of the 28th annual conference on Computer graphics and interactive techniques},
  pages={341--346},
  year={2001},
  organization={ACM}
}

@inproceedings{HitajAP17,
  author    = {Briland Hitaj and
               Giuseppe Ateniese and
               Fernando P{\'{e}}rez{-}Cruz},
  title     = {Deep Models Under the {GAN:} Information Leakage from Collaborative Deep Learning},
  booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} Conference on Computer and Communications Security, {CCS} 2017, Dallas, TX, USA, October 30 -
               November 03, 2017},
  pages     = {603--618},
  year      = {2017},
  crossref  = {DBLP:conf/ccs/2017},
  url       = {https://doi.org/10.1145/3133956.3134012},
  doi       = {10.1145/3133956.3134012},
  timestamp = {Tue, 06 Nov 2018 11:07:30 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/ccs/HitajAP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{KumarM17,
  author    = {Atul Kumar and
               Sameep Mehta},
  title     = {A Survey on Resilient Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1707.03184},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.03184},
  archivePrefix = {arXiv},
  eprint    = {1707.03184},
  timestamp = {Mon, 13 Aug 2018 16:48:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KumarM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{KurakinGB16,
  author    = {Alexey Kurakin and
               Ian J. Goodfellow and
               Samy Bengio},
  title     = {Adversarial Machine Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1611.01236},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01236},
  archivePrefix = {arXiv},
  eprint    = {1611.01236},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KurakinGB16a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PapernotMWJS16,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Xi Wu and
               Somesh Jha and
               Ananthram Swami},
  title     = {Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  booktitle = {{IEEE} Symposium on Security and Privacy, {SP} 2016, San Jose, CA, USA, May 22-26, 2016},
  pages     = {582--597},
  year      = {2016},
  crossref  = {DBLP:conf/sp/2016},
  url       = {https://doi.org/10.1109/SP.2016.41},
  doi       = {10.1109/SP.2016.41},
  timestamp = {Fri, 26 May 2017 00:50:06 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/sp/PapernotM0JS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PapernotMJFCS16,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Somesh Jha and
               Matt Fredrikson and
               Z. Berkay Celik and
               Ananthram Swami},
  title     = {The Limitations of Deep Learning in Adversarial Settings},
  booktitle = {{IEEE} European Symposium on Security and Privacy, EuroS{\&}P
               2016, Saarbr{\"{u}}cken, Germany, March 21-24, 2016},
  pages     = {372--387},
  year      = {2016},
  crossref  = {DBLP:conf/eurosp/2016},
  url       = {https://doi.org/10.1109/EuroSP.2016.36},
  doi       = {10.1109/EuroSP.2016.36},
  timestamp = {Wed, 24 May 2017 08:27:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/eurosp/PapernotMJFCS16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{PapernotMGJCS17,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Ian J. Goodfellow and
               Somesh Jha and
               Z. Berkay Celik and
               Ananthram Swami},
  title     = {Practical Black-Box Attacks against Machine Learning},
  booktitle = {Proceedings of the 2017 {ACM} on Asia Conference on Computer and Communications
               Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April 2-6,
               2017},
  pages     = {506--519},
  year      = {2017},
  crossref  = {DBLP:conf/ccs/2017asia},
  url       = {https://doi.org/10.1145/3052973.3053009},
  doi       = {10.1145/3052973.3053009},
  timestamp = {Tue, 06 Nov 2018 11:07:31 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/ccs/PapernotMGJCS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{SamangoueiKC18,
  author    = {Pouya Samangouei and
               Maya Kabkab and
               Rama Chellappa},
  title     = {Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  year      = {2018},
  crossref  = {DBLP:conf/iclr/2018},
  url       = {https://openreview.net/forum?id=BkJ3ibb0-},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/iclr/SamangoueiKC18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  
  @inproceedings{BarrenoNSJT06,
  author    = {Marco Barreno and
               Blaine Nelson and
               Russell Sears and
               Anthony D. Joseph and
               J. D. Tygar},
  title     = {Can machine learning be secure?},
  booktitle = {Proceedings of the 2006 {ACM} Symposium on Information,    Computer and Communications Security, {ASIACCS} 2006, Taipei, Taiwan, March 21-24, 2006},
  pages     = {16--25},
  year      = {2006},
  crossref  = {DBLP:conf/ccs/2006},
  url       = {https://doi.org/10.1145/1128817.1128824},
  doi       = {10.1145/1128817.1128824},
  timestamp = {Tue, 06 Nov 2018 11:07:30 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/ccs/BarrenoNSJT06},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BiggioNL12,
  author    = {Battista Biggio and
               Blaine Nelson and
               Pavel Laskov},
  title     = {Poisoning Attacks against Support Vector Machines},
  booktitle = {Proceedings of the 29th International Conference on Machine Learning {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  year      = {2012},
  crossref  = {DBLP:conf/icml/2012},
  url       = {http://icml.cc/2012/papers/880.pdf},
  timestamp = {Wed, 03 Apr 2019 17:43:35 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icml/BiggioNL12},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{KearnsL93,
  author    = {Michael J. Kearns and
               Ming Li},
  title     = {Learning in the Presence of Malicious Errors},
  journal   = {{SIAM} J. Comput.},
  volume    = {22},
  number    = {4},
  pages     = {807--837},
  year      = {1993},
  url       = {https://doi.org/10.1137/0222052},
  doi       = {10.1137/0222052},
  timestamp = {Sat, 27 May 2017 14:22:59 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/siamcomp/KearnsL93},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MeiZ15,
  author    = {Shike Mei and
               Xiaojin Zhu},
  title     = {Using Machine Teaching to Identify Optimal Training-Set Attacks on
               Machine Learners},
  booktitle = {Proceedings of the Twenty-Ninth {AAAI} Conference on Artificial Intelligence,
               January 25-30, 2015, Austin, Texas, {USA.}},
  pages     = {2871--2877},
  year      = {2015},
  crossref  = {DBLP:conf/aaai/2015},
  url       = {http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9472},
  timestamp = {Tue, 12 Jul 2016 21:51:14 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/MeiZ15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LyuHL15,
  author    = {Chunchuan Lyu and
               Kaizhu Huang and
               Hai{-}Ning Liang},
  title     = {A Unified Gradient Regularization Family for Adversarial Examples},
  booktitle = {2015 {IEEE} International Conference on Data Mining, {ICDM} 2015, Atlantic City, NJ, USA, November 14-17, 2015},
  pages     = {301--309},
  year      = {2015},
  crossref  = {DBLP:conf/icdm/2015},
  url       = {https://doi.org/10.1109/ICDM.2015.84},
  doi       = {10.1109/ICDM.2015.84},
  timestamp = {Mon, 07 Jan 2019 17:17:42 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/icdm/LyuHL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ShokriSSS17,
  author    = {Reza Shokri and
               Marco Stronati and
               Congzheng Song and
               Vitaly Shmatikov},
  title     = {Membership Inference Attacks Against Machine Learning Models},
  booktitle = {2017 {IEEE} Symposium on Security and Privacy, {SP} 2017, San Jose, CA, USA, May 22-26, 2017},
  pages     = {3--18},
  year      = {2017},
  crossref  = {DBLP:conf/sp/2017},
  url       = {https://doi.org/10.1109/SP.2017.41},
  doi       = {10.1109/SP.2017.41},
  timestamp = {Thu, 14 Feb 2019 16:15:00 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/sp/ShokriSSS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ShahamYN18,
  author    = {Uri Shaham and
               Yutaro Yamada and
               Sahand Negahban},
  title     = {Understanding adversarial training: Increasing local stability of
               supervised models through robust optimization},
  journal   = {Neurocomputing},
  volume    = {307},
  pages     = {195--204},
  year      = {2018},
  url       = {https://doi.org/10.1016/j.neucom.2018.04.027},
  doi       = {10.1016/j.neucom.2018.04.027},
  timestamp = {Thu, 21 Jun 2018 12:18:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/ijon/ShahamYN18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NarodytskaK17,
  author    = {Nina Narodytska and
               Shiva Prasad Kasiviswanathan},
  title     = {Simple Black-Box Adversarial Attacks on Deep Neural Networks},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition
               Workshops, {CVPR} Workshops 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {1310--1318},
  year      = {2017},
  crossref  = {DBLP:conf/cvpr/2017w},
  url       = {https://doi.org/10.1109/CVPRW.2017.172},
  doi       = {10.1109/CVPRW.2017.172},
  timestamp = {Wed, 21 Nov 2018 16:35:32 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/cvpr/NarodytskaK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{TramerKPBM17,
  author    = {Florian Tram{\`{e}}r and
               Alexey Kurakin and
               Nicolas Papernot and
               Dan Boneh and
               Patrick D. McDaniel},
  title     = {Ensemble Adversarial Training: Attacks and Defenses},
  journal   = {CoRR},
  volume    = {abs/1705.07204},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07204},
  archivePrefix = {arXiv},
  eprint    = {1705.07204},
  timestamp = {Mon, 13 Aug 2018 16:46:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/TramerKPBM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

}

@article{DBLP:journals/corr/XuEQ17,
  author    = {Weilin Xu and
               David Evans and
               Yanjun Qi},
  title     = {Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1704.01155},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.01155},
  archivePrefix = {arXiv},
  eprint    = {1704.01155},
  timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/XuEQ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rudin:1992:NTV:142273.142312,
 author = {Rudin, Leonid I. and Osher, Stanley and Fatemi, Emad},
 title = {Nonlinear Total Variation Based Noise Removal Algorithms},
 journal = {Phys. D},
 issue_date = {Nov. 1, 1992},
 volume = {60},
 number = {1-4},
 month = nov,
 year = {1992},
 issn = {0167-2789},
 pages = {259--268},
 numpages = {10},
 url = {https://doi.org/10.1016/0167-2789(92)90242-F},
 doi = {10.1016/0167-2789(92)90242-F},
 acmid = {142312},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
} 

@article{Chambolle:2004:ATV:964969.964985,
 author = {Chambolle, Antonin},
 title = {An Algorithm for Total Variation Minimization and Applications},
 journal = {J. Math. Imaging Vis.},
 issue_date = {January-March 2004},
 volume = {20},
 number = {1-2},
 month = jan,
 year = {2004},
 issn = {0924-9907},
 pages = {89--97},
 numpages = {9},
 url = {http://dx.doi.org/10.1023/B:JMIV.0000011325.36760.1e},
 doi = {10.1023/B:JMIV.0000011325.36760.1e},
 acmid = {964985},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {denoising, image reconstruction, mean curvature motion, total variation, zooming},
} 



@inproceedings{akhtar_defense_2018,
	address = {Salt Lake City, UT},
	title = {Defense {Against} {Universal} {Adversarial} {Perturbations}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578455/},
	doi = {10.1109/CVPR.2018.00357},
	abstract = {Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to ‘any’ image can fool a state-of-the-art network classiﬁer to change its prediction about the image label. These ‘Universal Adversarial Perturbations’ pose a serious threat to the success of Deep Learning in practice. We present the ﬁrst dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as ‘pre-input’ layers to a targeted model, such that the targeted model needs no modiﬁcation. The PRN is learned from real and synthetic image-agnostic perturbations, where an efﬁcient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is ﬁrst passed through the PRN and veriﬁed by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classiﬁers against unseen adversarial perturbations in the real-world scenarios with up to 97.5\% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.},
	language = {en},
	urldate = {2019-04-15},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Akhtar, Naveed and Liu, Jian and Mian, Ajmal},
	month = jun,
	year = {2018},
	pages = {3389--3398},
	file = {Akhtar et al. - 2018 - Defense Against Universal Adversarial Perturbation.pdf:/Users/mbenn/Zotero/storage/49Y2E9T8/Akhtar et al. - 2018 - Defense Against Universal Adversarial Perturbation.pdf:application/pdf}
}

@inproceedings{carlini_adversarial_2017,
	address = {New York, NY, USA},
	series = {{AISec} '17},
	title = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}: {Bypassing} {Ten} {Detection} {Methods}},
	isbn = {978-1-4503-5202-4},
	shorttitle = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}},
	url = {http://doi.acm.org/10.1145/3128572.3140444},
	doi = {10.1145/3128572.3140444},
	abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	urldate = {2019-04-15},
	booktitle = {Proceedings of the 10th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {ACM},
	author = {Carlini, Nicholas and Wagner, David},
	year = {2017},
	note = {event-place: Dallas, Texas, USA},
	pages = {3--14}
}

@inproceedings{khalid2019fademl,
  title={FAdeML: Understanding the Impact of Pre-Processing Noise Filtering on Adversarial Machine Learning},
  author={Khalid, Faiq and Hanif, Muhammad Abdullah and Rehman, Semeen and Qadir, Junaid and Shafique, Muhammad},
  booktitle={2019 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={902--907},
  year={2019},
  organization={IEEE}
}


@inproceedings{moosavi-dezfooli_universal_2017,
	title = {Universal {Adversarial} {Perturbations}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html},
	urldate = {2019-04-15},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	pages = {1765--1773},
	file = {Full Text PDF:/Users/mbenn/Zotero/storage/XB3ZTAMS/Moosavi-Dezfooli et al. - 2017 - Universal Adversarial Perturbations.pdf:application/pdf;Snapshot:/Users/mbenn/Zotero/storage/ZBS6LRVH/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html:text/html}
}

@article{perolat_playing_2018,
	title = {Playing the {Game} of {Universal} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1809.07802},
	abstract = {We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.},
	urldate = {2019-04-15},
	journal = {arXiv:1809.07802 [cs, stat]},
	author = {Perolat, Julien and Malinowski, Mateusz and Piot, Bilal and Pietquin, Olivier},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.07802},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1809.07802 PDF:/Users/mbenn/Zotero/storage/N5S5HBBU/Perolat et al. - 2018 - Playing the Game of Universal Adversarial Perturba.pdf:application/pdf;arXiv.org Snapshot:/Users/mbenn/Zotero/storage/RJTVBTPB/1809.html:text/html}
}

@article{shafahi_universal_2018,
	title = {Universal {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1811.11304},
	abstract = {Standard adversarial attacks change the predicted class label of an image by adding specially tailored small perturbations to its pixels. In contrast, a universal perturbation is an update that can be added to any image in a broad class of images, while still changing the predicted class label. We study the efficient generation of universal adversarial perturbations, and also efficient methods for hardening networks to these attacks. We propose a simple optimization-based universal attack that reduces the top-1 accuracy of various network architectures on ImageNet to less than 20\%, while learning the universal perturbation 13X faster than the standard method. To defend against these perturbations, we propose universal adversarial training, which models the problem of robust classifier generation as a two-player min-max game. This method is much faster and more scalable than conventional adversarial training with a strong adversary (PGD), and yet yields models that are extremely resistant to universal attacks, and comparably resistant to standard (per-instance) black box attacks. We also discover a rather fascinating side-effect of universal adversarial training: attacks built for universally robust models transfer better to other (black box) models than those built with conventional adversarial training.},
	urldate = {2019-04-15},
	journal = {arXiv:1811.11304 [cs]},
	author = {Shafahi, Ali and Najibi, Mahyar and Xu, Zheng and Dickerson, John and Davis, Larry S. and Goldstein, Tom},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.11304},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv\:1811.11304 PDF:/Users/mbenn/Zotero/storage/377IJ3AC/Shafahi et al. - 2018 - Universal Adversarial Training.pdf:application/pdf;arXiv.org Snapshot:/Users/mbenn/Zotero/storage/V3QSMCWP/1811.html:text/html}
}

@misc{EMNIST_data,
Author = {Gregory Cohen and Saeed Afshar and Jonathan Tapson and André van Schaik},
Title = {EMNIST: an extension of MNIST to handwritten letters},
Year = {2017},
Eprint = {arXiv:1702.05373},
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={AT\&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},
  volume={2},
  pages={18},
  year={2010}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}
