\subsection{Literature review}
Deep learning has altered the machine learning landscape by taking large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks pose great concern as it makes them vulnerable to adversarial attacks. Adversaries may manipulate legitimate inputs, which may be imperceptible to human eye, but can force a trained model to misclassify inputs and generate incorrect outputs.

One of the earliest work we know of is by Szegedy et al. \cite{Szegedy13} who discovered that well-performing neural networks are susceptible to adversarial attacks. It was  speculated that it was due to extreme non-linearity of neural networks, combined with insufficient model averaging and insufficient regularization of the purely supervised learning problem. 

Barreno et al. \cite{BarrenoNSJT06} first introduced the term \emph{poisoning attacks} which alters the training dataset by injecting, modifying or deleting points keeping the intention of modifying the decision boundaries of the targeted model following the work of Kearns et al. \cite{KearnsL93}, thus challenging the learning system's integrity. The poisoning attack of the training set can be performed in two ways: either by direct modification of the labels of the training data or by manipulating the input features depending on the capabilities posed by the adversary. In an offline learning settings, Biggio et al. \cite{BiggioNL12} implemented an attack that inserts inputs in the training set, which are crafted using a gradient ascent method. The method identifies the inputs corresponding to local maxima in the test error of the model. They presented a study which shows that by including these inputs into the training set one can result in a degraded classification accuracy for Support Vector Machines (SVMs) classifier at the testing time. Following their approach, Mei et al. \cite{MeiZ15} introduced a more general framework for poisoning. Their method finds out an optimal change to the training set whenever the targeted learning model is trained using a convex optimization loss such as SVMs or linear and logistic regression and its input domain is continuous.

It has been demonstrated \cite{Szegedy13,Goodfellow14, LyuHL15, ShokriSSS17, ShahamYN18} that adversarial training may assist to increase model robustness of neural networks by injecting adversarial examples into the training set. Adversarial training is a standard brute force approach where the defender simply generates a lot of adversarial examples and augments these perturbed data while training the targeted model. Adversarial training of a model is useful only on adversarial examples which are crafted on the original model. However, the defense is not robust for black-box attacks \cite{NarodytskaK17, PapernotMGJCS17} where an adversary generates malicious examples on a locally trained substitute model. Moreover, Tram{\`{e}}r et al. \cite{TramerKPBM17} have already proved that the adversarial training can be easily bypassed through a two-step attack, where random perturbations are applied to an instance first and then any classical attack technique is performed on it.

Hitaj et al. \cite{HitajAP17} exploited the real-time nature of the learning models to train a Generative Adversarial Network (GAN) and presented a GAN-based attack to extract information from honest victims in a collaborative deep learning framework. The goal of GAN is to produce samples identical to those in the training set without having access to the original training set. However, GAN-based method works only during the training phase in collaborative deep learning. The privacy of the collaborative systems could be jeopardised where an attack can be carried out in Convolutional Neural Networks even when the parameters are hidden using differential privacy. Goodfellow et al. \cite{Goodfellow14} proposed a way to perturb every input dimensions but with a small quantity in the direction of the sign of the gradient calculated using the Fast Gradient Sign Method (FGSM) method. This method efficiently minimizes the Euclidian distance between the original and the corresponding adversarial samples. Papernot et al. \cite{PapernotMJFCS16} choose to follow a more complicated process involving saliency maps to select only a limited number of input dimensions to perturb. The objective of using saliency map is to assign values to the combination of input dimensions which indicates whether the combination if perturbed, will contribute to the adversarial goals. This method effectively reduces the number of input features perturbed while crafting adversarial examples.

Since the findings of Szegedy \cite{Szegedy13}, a lot of attention has been drawn to the context of adversarial learning and the security of neural networks. A number of countermeasures have been proposed in recent years to mitigate the threats associated to adversarial attacks. Kurakin et al. \cite{KurakinGB16} proposed the technique of adversarial training to protect the learner by augmenting the training set using both original and perturbed data. Hinton et al. \cite{HintonVD15} introduced the concept of distillation which was used by Papernot et al. \cite{PapernotMWJS16} to propose a defensive mechanism against adversarial examples. Samangouei et al. \cite{SamangoueiKC18} proposed a mechanism to use Generative Adversarial Network as a countermeasure for adversarial perturbations, which works both for white box and black box attacks. In a typical GAN, a generative model which emulated the data distribution, and a discriminative model that differentiates between original input and perturbed input, are trained simultaneously. Although each of these proposed defense mechanisms were found to be efficient against particular classes of attacks, none of them could be used as a one-stop solution for all kinds of attacks. Moreover, implementation of these defense strategies can lead to degradation of performance and efficiency of the concerned model.